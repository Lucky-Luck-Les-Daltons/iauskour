binary: main
build_commit: bedb92b
build_number: 1268
cpu_has_arm_fma: false
cpu_has_avx: true
cpu_has_avx2: true
cpu_has_avx512: false
cpu_has_avx512_vbmi: false
cpu_has_avx512_vnni: false
cpu_has_blas: false
cpu_has_cublas: false
cpu_has_clblast: false
cpu_has_fma: true
cpu_has_gpublas: false
cpu_has_neon: false
cpu_has_f16c: true
cpu_has_fp16_va: false
cpu_has_wasm_simd: false
cpu_has_blas: false
cpu_has_sse3: true
cpu_has_vsx: false
debug: false
model_desc: LLaMA v2 7B mostly Q4_0
n_vocab: 32000  # output size of the final layer, 32001 for some models
optimize: true
time: 2023_09_29-11_08_31.237133103

###############
# User Inputs #
###############

alias: unknown # default: unknown
batch_size: 512 # default: 512
cfg_negative_prompt:
cfg_scale: 1,000000 # default: 1.0
chunks: -1 # default: -1 (unlimited)
color: false # default: false
ctx_size: 512 # default: 512
escape: false # default: false
export: false # default: false
file: # never logged, see prompt instead. Can still be specified for input.
frequency_penalty: 0,000000 # default: 0.0 
grammar:
grammar-file: # never logged, see grammar instead. Can still be specified for input.
hellaswag: false # default: false
hellaswag_tasks: 400 # default: 400
ignore_eos: false # default: false
in_prefix:
in_prefix_bos: false # default: false
in_suffix:
instruct: false # default: false
interactive: false # default: false
interactive_first: false # default: false
keep: 0 # default: 0
logdir: ./ # default: unset (no logging)
logit_bias:
lora: 
lora_base: 
low_vram: false # default: false
main_gpu: 0 # default: 0
memory_f32: false # default: false
mirostat: 0 # default: 0 (disabled)
mirostat_ent: 5,000000 # default: 5.0
mirostat_lr: 0,000000 # default: 0.1
mlock: false # default: false
model: ./models/7B/ggml-model-q4_0.gguf # default: models/7B/ggml-model.bin
model_draft:  # default:
multiline_input: false # default: false
n_gpu_layers: -1 # default: -1
n_predict: 10 # default: -1 (unlimited)
n_probs: 0 # only used by server binary, default: 0
no_mmap: false # default: false
no_mul_mat_q: false # default: false
no_penalize_nl: false # default: false
numa: false # default: false
ppl_output_type: 0 # default: 0
ppl_stride: 0 # default: 0
presence_penalty: 0,000000 # default: 0.0
prompt:
prompt_cache: 
prompt_cache_all: false # default: false
prompt_cache_ro: false # default: false
prompt_tokens:
random_prompt: false # default: false
repeat_penalty: 1,000000 # default: 1.1
reverse_prompt:
rope_freq_base: 10000,000000 # default: 10000.0
rope_freq_scale: 1,000000 # default: 1.0
seed: 1695554224 # default: -1 (random seed)
simple_io: false # default: false
temp: 0,000000 # default: 0.8
tensor_split: [0,000000e+00]
tfs: 1,000000 # default: 1.0
threads: 4 # default: 4
top_k: 40 # default: 40
top_p: 0,000000 # default: 0.95
typical_p: 1,000000 # default: 1.0
verbose_prompt: false # default: false

######################
# Generation Results #
######################

output: " hopefully, the next time I'll be able"
output_tokens: [1, 27581, 29892, 278, 2446, 931, 306, 29915, 645, 367, 2221]

###########
# Timings #
###########

mst_eval: 418,02  # ms / token during generation
mst_p_eval: -nan  # ms / token during prompt processing
mst_sample: 0,06  # ms / token during sampling
n_eval: 10  # number of tokens generated (excluding the first one)
n_p_eval: 0  # number of tokens processed in batches at the beginning
n_sample: 10  # number of sampled tokens
t_eval_us: 4180239  # total microseconds spent generating tokens
t_load_us: 1188750  # total microseconds spent loading the model
t_p_eval_us: 0  # total microseconds spent prompt processing
t_sample_us: 639  # total microseconds spent sampling
ts_eval: 2,39  # tokens / second during generation
ts_p_eval: -nan  # tokens / second during prompt processing
ts_sample: 15649,45  # tokens / second during sampling
